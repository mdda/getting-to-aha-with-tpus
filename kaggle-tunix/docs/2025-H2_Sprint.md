## 2025-H2 AI Sprint (GDE)

* GCP Credits  (2 weeks duration)
  + Redeem before `18-Dec-2025`
* Due `21-Dec-2025`

### Title 
Evaluate memory usage of different LLMs/'modes' on TPUv5-8

### Description
The Tunix Kaggle competition is looking for efficiency in training a Gemma 1B/2B model 
to become a good general reasoner..  I will explore how this inference/training 
(along with other interesting dimensions) can be spread onto the Kaggle TPU instance.

### Which will be your project topic?
* JAX/Keras/TPU: Tutorial or demo on model development with JAX or Keras latest features

### List additional products you will be using in the project
* Gemini 3, JAX, TPU, Vertex AI	

### Output format 
#### (Select all that applies to your project)
* Sample code
* (will likely do blog post too - but no commitment)

### Promotion
* Please add this clause to your blog or github repo:
  + "Google Cloud credits are provided for this project." 
  + with `#AISprint` hashtag
* Share the result on social media like X/LinkedIn (or any public social media) 
  + with `#AISprintH2` hashtag



## TABS

* [Google Tunix Hack - Train a model to show its work | Kaggle](https://www.kaggle.com/competitions/google-tunix-hackathon/overview)
* [Tunix Competition notes - Google Docs](https://docs.google.com/document/d/1Vnjfy5KcA-9Mme2Y_Fg2N41FvuBeys9Lu-660_ooz0o/edit?tab=t.0#heading=h.10jl1mcsufh1)
* [Google | Gemma 3 | Kaggle](https://www.kaggle.com/models/google/gemma-3/flax/gemma3-1b-it)
* [New Tab](chrome://newtab/)
* [DSA-CAST-TUNIX-noLoRA-from-scratch](https://www.kaggle.com/code/danielwycoff/dsa-cast-tunix-nolora-from-scratch)
* [Supervised Fine Tuning Full](https://www.kaggle.com/code/marculera/supervised-fine-tuning-full)
* [grpo_demo Gemma2 2B](https://www.kaggle.com/code/windmaple/grpo-demo-gemma2-2b#Hyperparameters)
* [JupyterLab (auto-z)](http://localhost:8181/lab/workspaces/auto-z/tree/OpenSource/getting-to-aha-with-tpus/kaggle-tunix)
* [xlocalhost](http://xlocalhost:8585/lab/workspaces/auto-Z/tree/getting-to-aha-with-tpus/kaggle-tunix)
* [The Training Cookbook â€” JAX documentation](https://docs.jax.dev/en/latest/the-training-cookbook.html)
* [tunix/tunix/models/gemma3/model.py at main Â· google/tunix](https://github.com/google/tunix/blob/main/tunix/models/gemma3/model.py#L295)
* [tunix/tunix/distillation/strategies/feature_projection.py at main Â· google/tunix](https://github.com/google/tunix/blob/main/tunix/distillation/strategies/feature_projection.py)
* [LoRA & QLoRA Demo â€” Tunix documentation](https://tunix.readthedocs.io/en/latest/_collections/examples/qlora_gemma.html#apply-lora-qlora-to-the-base-model)
* [getting-to-aha-with-tpus/kaggle-tunix/1-mem-and-perf.py at main Â· mdda/getting-to-aha-with-tpus](https://github.com/mdda/getting-to-aha-with-tpus/blob/main/kaggle-tunix/1-mem-and-perf.py)
* [python - How to save a Seaborn plot into a file - Stack Overflow](https://stackoverflow.com/questions/32244753/how-to-save-a-seaborn-plot-into-a-file)


* [on-policy distillation for rlvr tasks - Google Search](https://www.google.com/search?q=on-policy+distillation+for+rlvr+tasks&oq=on-policy+distillation+for+rlvr+tasks&gs_lcrp=EgZjaHJvbWUyBggAEEUYOTIKCAEQABiABBiiBDIHCAIQABjvBTIHCAMQABjvBTIKCAQQABiiBBiJBTIHCAUQABjvBdIBCTExNDM5ajBqN6gCALACAA&sourceid=chrome&ie=UTF-8)
* [arxiv.org/pdf/2306.13649](https://arxiv.org/pdf/2306.13649)
* [On-Policy Distillation - Thinking Machines Lab](https://thinkingmachines.ai/blog/on-policy-distillation/)
* [(2) Lewis Tunstall on X: "ðŸŒŸ Introducing General On-Policy Logit Distillation ðŸŒŸ Inspired by the latest from @thinkymachines, we extend on-policy distillation to enable ANY teacher to be distilled into ANY student, even if their tokenizers differ! We've added this to TRL so you can now take any pair of https://t.co/XOAXnLbbbk" / X](https://x.com/_lewtun/status/1983620843952328726)
* [Train a GPT2 model with JAX on TPU for free - Google Developers Blog](https://developers.googleblog.com/train-gpt2-model-with-jax-on-tpu/)
* [New Tab](chrome://newtab/)

