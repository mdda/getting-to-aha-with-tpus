## 2025-H2 AI Sprint (GDE)

* GCP Credits  (2 weeks duration)
  + Redeem before `18-Dec-2025`
* Due `21-Dec-2025`

### Title 
Evaluate memory usage of different LLMs/'modes' on TPUv5-8

### Description
The Tunix Kaggle competition is looking for efficiency in training a Gemma 1B/2B model 
to become a good general reasoner..  I will explore how this inference/training 
(along with other interesting dimensions) can be spread onto the Kaggle TPU instance.

### Which will be your project topic?
* JAX/Keras/TPU: Tutorial or demo on model development with JAX or Keras latest features

### List additional products you will be using in the project
* Gemini 3, JAX, TPU, Vertex AI	

### Output format 
#### (Select all that applies to your project)
* Sample code
* (will likely do blog post too - but no commitment)

### Promotion
* Please add this clause to your blog or github repo:
  + "Google Cloud credits are provided for this project." 
  + with `#AISprint` hashtag
* Share the result on social media like X/LinkedIn (or any public social media) 
  + with `#AISprintH2` hashtag
